{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral CNN\n",
    "\n",
    "This code is to train the Hyperspectral CNN. Warning: You need at least 18GB of RAM, to process the TfRecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sallinen/Programmation/predicting-poverty-through-time/src\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from lib.tfrecordhelper import TfrecordHelper\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str):\n",
    "    \"\"\"\n",
    "    Helper to load dataset\n",
    "\n",
    "    Args:\n",
    "    - path (str): Path to dataset\n",
    "\n",
    "    Returns:\n",
    "    - dic which contains all data\n",
    "    \"\"\"\n",
    "    tf_helper = TfrecordHelper(path, ls_bands=\"ms\", nl_band=\"viirs\")\n",
    "    input_dic = {}\n",
    "    tf_helper.keyword_lat = \"lat\"\n",
    "    tf_helper.keyword_lon = \"lon\"\n",
    "    tf_helper.process_dataset()\n",
    "    for i, feature in enumerate(tf_helper.dataset):\n",
    "        input_dic[i] = {\n",
    "        \"year\": feature[\"years\"].numpy(),\n",
    "        \"cluster_lat\": feature[\"locs\"].numpy()[0],\n",
    "        \"cluster_lon\": feature[\"locs\"].numpy()[1],\n",
    "        \"img\": (feature[\"images\"][:,:,:7].numpy()),\n",
    "        \"nightlight\": np.mean(feature[\"images\"][:,:,7].numpy()),\n",
    "    }\n",
    "    \n",
    "    # Remove data where entry is broken (one channel contains only zeros)\n",
    "    remove = []\n",
    "    for feature in tqdm(input_dic):\n",
    "        if input_dic[feature][\"nightlight\"] == 0:\n",
    "            remove.append(feature)\n",
    "            continue\n",
    "        for dim in input_dic[feature][\"img\"]:\n",
    "            if not np.any(dim):\n",
    "                remove.append(feature)\n",
    "                break\n",
    "    \n",
    "    for r in remove:\n",
    "        input_dic.pop(r)\n",
    "    return input_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/tfrecords/raw/\"\n",
    "files = os.listdir(path) # path to the processed tfrecords from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 20:13:32.365911: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 20:13:32.366942: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/781 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d384a39c0a614f7e908a9d73904272e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/669 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abc37e8204c04f328a9cce13ab542fc2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/419 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec877ddb93934d82a3abb884a14d7a44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/503 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15be5b41addb4976ad2a3dfa478ed0e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/475 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97463ae34d8c4a49b34065bd6cdbab6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/645 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58b1429d06ba4bdd92a6b03f04d25bb1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1611 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0065b6f6597410bbe144bbaee5f4fe9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/710 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf1378f9c3854d59924c1963a20f573a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/516 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2430908219ff4f7d9386e46295ca064c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/525 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9552ca466f6d477692f8d33dd1b73a18"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_dics = [] # will contain all information\n",
    "for file in files:\n",
    "    raw_path = path + file\n",
    "    data = load_dataset(raw_path)\n",
    "    input_dics.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "deeb44e0bb3541d5b915f8ba02b83acc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "years = []\n",
    "lat = []\n",
    "lon = []\n",
    "for country in tqdm(input_dics):\n",
    "    data = country\n",
    "    for feature in data:\n",
    "        years.append(data[feature][\"year\"])\n",
    "        lat.append(data[feature][\"cluster_lat\"])\n",
    "        lon.append(data[feature][\"cluster_lon\"])\n",
    "        data[feature][\"img\"][:3,:,:] *=3 # RGB images to dark, got better performance by using it\n",
    "        X.append(data[feature][\"img\"])\n",
    "        y.append(data[feature][\"nightlight\"])\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [np.mean(X[:,i,:,:]) for i in range(7)]\n",
    "stds = [np.std(X[:,i,:,:]) for i in range(7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=means, std=stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bins for nighttime images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nightlights_to_class(data):\n",
    "    \"\"\"\n",
    "    Data are labels. Perform GMM based on the input and creates 5 classes out of it.\n",
    "\n",
    "    Args:\n",
    "    - data: radiance (nighttime images)\n",
    "\n",
    "    Return:\n",
    "    - list of labels\n",
    "    \"\"\"\n",
    "    x = data.reshape(-1,1)\n",
    "    gmm = GMM(n_components=5).fit(x)\n",
    "    labels = gmm.predict(x)\n",
    "    cut_label1 = data[labels==0].max()\n",
    "    cut_label2 = data[labels==1].max()\n",
    "    cut_label3 = data[labels==2].max()\n",
    "    cut_label4 = data[labels==3].max()\n",
    "    cut_label5 = data[labels==4].max()\n",
    "    cutoffs = [cut_label1, cut_label2, cut_label3,  cut_label4, cut_label5]\n",
    "    cutoffs = sorted(cutoffs)\n",
    "    \n",
    "    y_labels = []\n",
    "    for d in data:\n",
    "        if d <= cutoffs[0]:\n",
    "            y_labels.append(0)\n",
    "        elif d <= cutoffs[1]:\n",
    "            y_labels.append(1)\n",
    "        elif d <= cutoffs[2]:\n",
    "            y_labels.append(2)\n",
    "        elif d <= cutoffs[3]:\n",
    "            y_labels.append(3)\n",
    "        else:\n",
    "            y_labels.append(4)\n",
    "    return np.array(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = nightlights_to_class(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = data\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x) # transpose is required by PyTorch\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(X, y_labels, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(dataset)))\n",
    "split = int(np.floor(.4 * len(dataset)))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
    "                                                sampler=valid_sampler)\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"val\": validation_loader\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    \"train\": len(train_sampler),\n",
    "    \"val\": len(valid_sampler)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sallinen/.var/app/com.jetbrains.PyCharm-Professional/cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/sallinen/miniconda3/envs/predicting-poverty-through-time/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sallinen/miniconda3/envs/predicting-poverty-through-time/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True) # load resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperspectral Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = nn.Conv2d(7, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), dilation=1, bias=False)\n",
    "model.conv1 = new_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/18\n",
      "----------\n",
      "train Loss: 1.1592 Acc: 0.5138\n",
      "val Loss: 1.0501 Acc: 0.5261\n",
      "\n",
      "Epoch 1/18\n",
      "----------\n",
      "train Loss: 0.9543 Acc: 0.5873\n",
      "val Loss: 1.1457 Acc: 0.4486\n",
      "\n",
      "Epoch 2/18\n",
      "----------\n",
      "train Loss: 0.8960 Acc: 0.6116\n",
      "val Loss: 1.0682 Acc: 0.5375\n",
      "\n",
      "Epoch 3/18\n",
      "----------\n",
      "train Loss: 0.8472 Acc: 0.6314\n",
      "val Loss: 1.1126 Acc: 0.4442\n",
      "\n",
      "Epoch 4/18\n",
      "----------\n",
      "train Loss: 0.7948 Acc: 0.6556\n",
      "val Loss: 1.1716 Acc: 0.4710\n",
      "\n",
      "Epoch 5/18\n",
      "----------\n",
      "train Loss: 0.7653 Acc: 0.6574\n",
      "val Loss: 1.5955 Acc: 0.4008\n",
      "\n",
      "Epoch 6/18\n",
      "----------\n",
      "train Loss: 0.7178 Acc: 0.6931\n",
      "val Loss: 1.1291 Acc: 0.5158\n",
      "\n",
      "Epoch 7/18\n",
      "----------\n",
      "train Loss: 0.6569 Acc: 0.7325\n",
      "val Loss: 1.2063 Acc: 0.5048\n",
      "\n",
      "Epoch 8/18\n",
      "----------\n",
      "train Loss: 0.6365 Acc: 0.7404\n",
      "val Loss: 1.2779 Acc: 0.5165\n",
      "\n",
      "Epoch 9/18\n",
      "----------\n",
      "train Loss: 0.6287 Acc: 0.7431\n",
      "val Loss: 1.4686 Acc: 0.5327\n",
      "\n",
      "Epoch 10/18\n",
      "----------\n",
      "train Loss: 0.6196 Acc: 0.7455\n",
      "val Loss: 3.1947 Acc: 0.3997\n",
      "\n",
      "Epoch 11/18\n",
      "----------\n",
      "train Loss: 0.6116 Acc: 0.7509\n",
      "val Loss: 1.4220 Acc: 0.5386\n",
      "\n",
      "Epoch 12/18\n",
      "----------\n",
      "train Loss: 0.6081 Acc: 0.7526\n",
      "val Loss: 3.4043 Acc: 0.3986\n",
      "\n",
      "Epoch 13/18\n",
      "----------\n",
      "train Loss: 0.5984 Acc: 0.7607\n",
      "val Loss: 1.1381 Acc: 0.5452\n",
      "\n",
      "Epoch 14/18\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.7624\n",
      "val Loss: 2.9379 Acc: 0.4012\n",
      "\n",
      "Epoch 15/18\n",
      "----------\n",
      "train Loss: 0.5911 Acc: 0.7634\n",
      "val Loss: 1.1890 Acc: 0.5386\n",
      "\n",
      "Epoch 16/18\n",
      "----------\n",
      "train Loss: 0.5900 Acc: 0.7629\n",
      "val Loss: 1.2361 Acc: 0.5338\n",
      "\n",
      "Epoch 17/18\n",
      "----------\n",
      "train Loss: 0.5931 Acc: 0.7627\n",
      "val Loss: 1.2669 Acc: 0.5209\n",
      "\n",
      "Epoch 18/18\n",
      "----------\n",
      "train Loss: 0.5888 Acc: 0.7695\n",
      "val Loss: 1.2522 Acc: 0.5334\n",
      "\n",
      "Training complete in 3m 47s\n",
      "Best val Acc: 0.545187\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'model_weights_all_countries_multichannel_{time.time()}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmodel = torch.nn.Sequential(*list(model_ft.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    nmodel.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/781 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59d024bda2bd40a687766078dd4dc9b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/669 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1138fa615b24a71b1c8cd7440279bf2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/419 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ebea3462f4d4c799b43c013348cd27a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/479 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b231183c27f447c9ba58413674b407d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/475 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3475d9b7c19248ec80eed9b2da71536d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/645 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d433615f16e84cf08deaf30eb55677e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1588 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "816ef2a399c9494f8a4d45000c51a4c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/708 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d81586d968fd47fea78ea9b4f7ee147a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/516 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d7ab49305e9425da5b81d79ea3a60a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/525 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0987226d00bb4f58847bb37b1e6cb61a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data in input_dics:\n",
    "    for feature in tqdm(data, total=len(data)):\n",
    "        input_batch = preprocess(data[feature]['img']).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = nmodel(input_batch.to('cuda'))\n",
    "        data[feature][\"feature\"] = np.squeeze(output.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge of weights and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/781 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39f3423c853040ebadb8b8afae27f67b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373/3038713665.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(tmp)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/669 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98ab4da2b26a44e59fc2d9c52d47dad5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/419 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b865b47a67543498551154a1c09ba40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/479 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d049d05117bc4b81b60d36d751603332"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373/3038713665.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(tmp)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/475 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f086a49154ed4ec5a94ae41c09e8b1fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373/3038713665.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(tmp)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/645 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7cf1ef8a4919406caf85103b24771e44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373/3038713665.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(tmp)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1588 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2423b03e988c4b939642eacc9fbe866a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373/3038713665.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(tmp)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/708 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5719d54b52974f9896116fe186c1e8a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373/3038713665.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(tmp)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/516 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f534f31b3684e3a9b582d9a9c42c272"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373/3038713665.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(tmp)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/525 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4ed13a4f9f94865b4b2bc5f21a9e5d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373/3038713665.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(tmp)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for data in input_dics:\n",
    "    years = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "    features = []\n",
    "    nightlights = []\n",
    "    for feature in tqdm(data, total=len(data)):\n",
    "        years.append(data[feature][\"year\"])\n",
    "        lat.append(data[feature][\"cluster_lat\"])\n",
    "        lon.append(data[feature][\"cluster_lon\"])\n",
    "        features.append(data[feature][\"feature\"].numpy().tolist())\n",
    "        nightlights.append(data[feature][\"nightlight\"])\n",
    "    tmp = pd.DataFrame.from_dict({\"year\": years, \"lat\": lat, 'lon': lon, \"features\": features, \"nightlight\": nightlights})\n",
    "    df = df.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/cnn_features/resnet_trans_all_countries_hyper.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
