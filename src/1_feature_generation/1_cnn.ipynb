{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral CNN\n",
    "\n",
    "This code is to train the Hyperspectral CNN. Warning: You need at least 18GB of RAM, to process the TfRecords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sallinen/Programmation/predicting-poverty-through-time/src\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from lib.tfrecordhelper import TfrecordHelper\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#added for reproducibility\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(0)\n",
    "import random\n",
    "random.seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str):\n",
    "    \"\"\"\n",
    "    Helper to load dataset\n",
    "\n",
    "    Args:\n",
    "    - path (str): Path to dataset\n",
    "\n",
    "    Returns:\n",
    "    - dic which contains all data\n",
    "    \"\"\"\n",
    "    tf_helper = TfrecordHelper(path, ls_bands=\"ms\", nl_band=\"viirs\")\n",
    "    input_dic = {}\n",
    "    tf_helper.keyword_lat = \"lat\"\n",
    "    tf_helper.keyword_lon = \"lon\"\n",
    "    tf_helper.process_dataset()\n",
    "    for i, feature in enumerate(tf_helper.dataset):\n",
    "        input_dic[i] = {\n",
    "        \"year\": feature[\"years\"].numpy(),\n",
    "        \"cluster_lat\": feature[\"locs\"].numpy()[0],\n",
    "        \"cluster_lon\": feature[\"locs\"].numpy()[1],\n",
    "        \"img\": (feature[\"images\"][:,:,:7].numpy()),\n",
    "        \"nightlight\": np.mean(feature[\"images\"][:,:,7].numpy()),\n",
    "    }\n",
    "    \n",
    "    # Remove data where entry is broken (one channel contains only zeros)\n",
    "    remove = []\n",
    "    for feature in tqdm(input_dic):\n",
    "        if input_dic[feature][\"nightlight\"] == 0:\n",
    "            remove.append(feature)\n",
    "            continue\n",
    "        for dim in input_dic[feature][\"img\"]:\n",
    "            if not np.any(dim):\n",
    "                remove.append(feature)\n",
    "                break\n",
    "    \n",
    "    for r in remove:\n",
    "        input_dic.pop(r)\n",
    "    return input_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/tfrecords/raw/\"\n",
    "files = os.listdir(path) # path to the processed tfrecords from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 21:54:50.344900: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 21:54:50.345884: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/781 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37649309ff5644c5b3a14c8fb27efa1e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/669 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f64e8ff19475496cbcc6608c2197bd4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/419 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea3674bfd86e4f6a8cff010cef61dbb9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/503 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d058d246f3c4c46b0c84886bd845e00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/475 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "087d92d0da8044c69f3636dd83723598"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/645 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "270f1fda94394f559d01839830a712f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1611 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6d8362eab764c28a449166e0cdb660d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/710 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8b7648cadaa4029919fa8fb118da0d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/516 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84c1cbaf047b4cf9bbeae59eff3689fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/525 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3762787bf17466fa24871307b572246"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_dics = [] # will contain all information\n",
    "for file in files:\n",
    "    raw_path = path + file\n",
    "    data = load_dataset(raw_path)\n",
    "    input_dics.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "720d14f0e99b48fd9ff99336c670c268"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "years = []\n",
    "lat = []\n",
    "lon = []\n",
    "for country in tqdm(input_dics):\n",
    "    data = country\n",
    "    for feature in data:\n",
    "        years.append(data[feature][\"year\"])\n",
    "        lat.append(data[feature][\"cluster_lat\"])\n",
    "        lon.append(data[feature][\"cluster_lon\"])\n",
    "        data[feature][\"img\"][:3,:,:] *=3 # RGB images to dark, got better performance by using it\n",
    "        X.append(data[feature][\"img\"])\n",
    "        y.append(data[feature][\"nightlight\"])\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [np.mean(X[:,i,:,:]) for i in range(7)]\n",
    "stds = [np.std(X[:,i,:,:]) for i in range(7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import lib.clusters_utils as cl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "cl.save_lat_lon(lat,lon)\n",
    "indices = cl.split_k_sets(2, lat=lat, lon=lon)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cl.plot_split_perf(indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=means, std=stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bins for nighttime images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nightlights_to_class(data, model, n_components):\n",
    "    \"\"\"\n",
    "    Data are labels. Perform GMM based on the input and creates 5 classes out of it.\n",
    "\n",
    "    Args:\n",
    "    - data: radiance (nighttime images)\n",
    "\n",
    "    Return:\n",
    "    - list of labels\n",
    "    \"\"\"\n",
    "    x = data.reshape(-1,1)\n",
    "    model_ = model.fit(x)\n",
    "    labels = model_.predict(x)\n",
    "\n",
    "    if(model.converged_):\n",
    "        print(\"there is convergence in \")\n",
    "        print(model.n_iter_)\n",
    "    else :\n",
    "        print(\"no convergence\")\n",
    "    cutoffs = []\n",
    "    for i in range(n_components):\n",
    "        cutoffs.append(data[labels==i].max())\n",
    "    cutoffs = sorted(cutoffs)\n",
    "\n",
    "    y_labels = []\n",
    "\n",
    "    for d in data:\n",
    "        for i in range(n_components):\n",
    "            if d <= cutoffs[i]:\n",
    "                y_labels.append(i)\n",
    "                break\n",
    "\n",
    "    return np.array(y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is convergence in \n",
      "9\n"
     ]
    }
   ],
   "source": [
    "n_components = 5\n",
    "#y_labels = nightlights_to_class(y,GMM(n_components=n_components, n_init=1, random_state=2), n_components=n_components) #0.562087\n",
    "#y_labels = nightlights_to_class(y,GMM(n_components=n_components, n_init=1, random_state=2, init_params='k-means++'), n_components=n_components) #0.549596\n",
    "#y_labels = nightlights_to_class(y,GMM(n_components=n_components, n_init=10, init_params='k-means++', random_state=2), n_components=n_components) #0.557678\n",
    "#y_labels = nightlights_to_class(y,GMM(n_components=n_components, n_init=10, random_state=2), n_components=n_components) #0.534166\n",
    "\n",
    "#y_labels = nightlights_to_class(y,GMM(n_components=n_components, n_init=1, random_state=1), n_components=n_components) #0.562454\n",
    "y_labels = nightlights_to_class(y,GMM(n_components=n_components, random_state=1), n_components=n_components) #0.566495\n",
    "#y_labels = nightlights_to_class(y,GMM(n_components=n_components, n_init=10, init_params='k-means++', random_state=1), n_components=n_components) #0.547759\n",
    "#y_labels = nightlights_to_class(y,GMM(n_components=n_components, n_init=10, random_state=1), n_components=n_components) #0.562454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = data\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x) # transpose is required by PyTorch\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(X, y_labels, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices = list(range(len(dataset)))\n",
    "#split = int(np.floor(.4 * len(dataset)))\n",
    "train_indices, val_indices = indices[0], indices[1]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=128, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
    "                                                sampler=valid_sampler)\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"val\": validation_loader\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    \"train\": len(train_sampler),\n",
    "    \"val\": len(valid_sampler)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "3267"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_indices)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sallinen/.var/app/com.jetbrains.PyCharm-Professional/cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/sallinen/miniconda3/envs/predicting-poverty-through-time/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sallinen/miniconda3/envs/predicting-poverty-through-time/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True) # load resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperspectral Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = nn.Conv2d(7, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), dilation=1, bias=False)\n",
    "model.conv1 = new_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 5)\n",
    "# (Crop size) (Final layer NN)\n",
    "# (224x224) (100, 200, 200, 5) -> 0.570536\n",
    "# (224x224) (5)                -> 0.549963 / 0.562087\n",
    "\n",
    "# (255x255) (5)                -> 0.561352\n",
    "# (255x255) (100, 200, 200, 5) -> 0.540044\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/18\n",
      "----------\n",
      "train Loss: 1.2184 Acc: 0.4853\n",
      "val Loss: 1.3335 Acc: 0.4187\n",
      "\n",
      "Epoch 1/18\n",
      "----------\n",
      "train Loss: 1.0069 Acc: 0.5534\n",
      "val Loss: 1.4799 Acc: 0.4356\n",
      "\n",
      "Epoch 2/18\n",
      "----------\n",
      "train Loss: 0.9601 Acc: 0.5822\n",
      "val Loss: 1.5737 Acc: 0.4294\n",
      "\n",
      "Epoch 3/18\n",
      "----------\n",
      "train Loss: 0.9098 Acc: 0.6034\n",
      "val Loss: 1.6326 Acc: 0.4200\n",
      "\n",
      "Epoch 4/18\n",
      "----------\n",
      "train Loss: 0.8677 Acc: 0.6150\n",
      "val Loss: 2.1807 Acc: 0.1530\n",
      "\n",
      "Epoch 5/18\n",
      "----------\n",
      "train Loss: 0.8412 Acc: 0.6444\n",
      "val Loss: 1.7180 Acc: 0.4288\n",
      "\n",
      "Epoch 6/18\n",
      "----------\n",
      "train Loss: 0.7920 Acc: 0.6600\n",
      "val Loss: 1.0877 Acc: 0.4242\n",
      "\n",
      "Epoch 7/18\n",
      "----------\n",
      "train Loss: 0.7442 Acc: 0.6967\n",
      "val Loss: 1.1073 Acc: 0.4692\n",
      "\n",
      "Epoch 8/18\n",
      "----------\n",
      "train Loss: 0.7188 Acc: 0.6995\n",
      "val Loss: 1.0774 Acc: 0.5139\n",
      "\n",
      "Epoch 9/18\n",
      "----------\n",
      "train Loss: 0.7112 Acc: 0.7089\n",
      "val Loss: 1.4899 Acc: 0.4282\n",
      "\n",
      "Epoch 10/18\n",
      "----------\n",
      "train Loss: 0.7078 Acc: 0.7131\n",
      "val Loss: 1.1200 Acc: 0.4475\n",
      "\n",
      "Epoch 11/18\n",
      "----------\n",
      "train Loss: 0.7019 Acc: 0.7097\n",
      "val Loss: 1.2923 Acc: 0.5115\n",
      "\n",
      "Epoch 12/18\n",
      "----------\n",
      "train Loss: 0.6822 Acc: 0.7250\n",
      "val Loss: 1.1552 Acc: 0.3909\n",
      "\n",
      "Epoch 13/18\n",
      "----------\n",
      "train Loss: 0.6864 Acc: 0.7219\n",
      "val Loss: 1.1613 Acc: 0.4105\n",
      "\n",
      "Epoch 14/18\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ft"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'model_weights_all_countries_multichannel_{time.time()}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "nmodel = torch.nn.Sequential(*list(model_ft.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    nmodel.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for data in input_dics:\n",
    "    for feature in tqdm(data, total=len(data)):\n",
    "        input_batch = preprocess(data[feature]['img']).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = nmodel(input_batch.to('cuda'))\n",
    "        data[feature][\"feature\"] = np.squeeze(output.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge of weights and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for data in input_dics:\n",
    "    years = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "    features = []\n",
    "    nightlights = []\n",
    "    for feature in tqdm(data, total=len(data)):\n",
    "        years.append(data[feature][\"year\"])\n",
    "        lat.append(data[feature][\"cluster_lat\"])\n",
    "        lon.append(data[feature][\"cluster_lon\"])\n",
    "        features.append(data[feature][\"feature\"].numpy().tolist())\n",
    "        nightlights.append(data[feature][\"nightlight\"])\n",
    "    tmp = pd.DataFrame.from_dict({\"year\": years, \"lat\": lat, 'lon': lon, \"features\": features, \"nightlight\": nightlights})\n",
    "    df = df.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/cnn_features/resnet_trans_all_countries_hyper.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
